
```ad-tip
官方文档 ✅ csdn ❌
```

# 简历

**开始时间-结束时间**：2024-4-22～至今

**项目介绍：** 基于 Spring Cloud + Spring Security + MyBatis-Plus 的药品知识库后台项目，为公司现有产品“药视通”和“药费通”提供数据服务，能够全维度展示药品、批次以及批次内药品信息，并对数据进行精细化管理，实现对全国和属地知识库版本的全生命周期管理（提交、审批、发布、生效），此外支持认证登录、账户管理、权限隔离、审核中心、文件导入、操作记录、知识包生成等功能。目前产品已经推广到全国 32 个省份的多家医院。

**个人工作：**
- 在 mentor 指导下独立完成项目从 0-1 开发、库表设计、接口设计，与前端、测试、产品、运维配合并顺利在项目排期内完成产品的评审、发版、迭代和交付。
- springcloud #todo
- 基于 Spring Security 实现用户入驻和授权。
- 多版本知识库管理中，考虑数据量会对查询效率、并发下性能以及删除表数据产生影响，所以按数据的版本进行分表，用户新建版本会创建对应的分表，从而实现版本操作隔离。
- 基于线程池、CountDownLatch 和 原子类 实现异步的版本创建和文件导入，并显示进度。优化 2w 页医保药品 pdf 导入（实测从 10h 优化到 1.5h）
	- 停止文件导入 #todo
- 在 mysql 百万数据量下对分页、group by、多表 join 进行 sql 优化
- 基于 mysql 锁保证版本内批次名称唯一，并实现分布式锁保证所有版本串行创建
	- 丢失写
- OOM

# 依赖版本

- Springboot 2.7.13
- Springcloud-alibaba 2021.0.5.0
- Nacos-client 2.2.0

# 库表设计

## 版本分表
单个版本中表的数据量（需要分表的表）

- 药品表（40w+）
- 批次表（1.5w）
- 药品-批次关联表（100w+）
:::info
**为什么不同版本库的药品表、批次表、药品-批次关联表要用时间戳后缀分表，而不是直接在一张表中用版本id字段区分？**
:::
全国+属地一共32个地区，在每月各发布一个新版本的情况下，关联表会多出3200w+条数据

- 考虑对查询效率的影响，按数据的版本进行分表，用户新建版本后会基于三张模板表创建对应的分表，不同版本的分表之间通过时间戳后缀进行区分
- 考虑旧版本数据迁移后的删除问题，方便删版本
   - 一张表中删除数据会加锁，加了行锁以后其他事务就不能加表锁（保证批次名称唯一需要加表锁）
   - innodb引擎删除大量数据会导致内存碎片，直接删除表来删除数据更方便 todo
   **mysql删除表数据与优化表空间**在MyISAM和InnoDB存储引擎中，数据信息存在在单个文件中。删除表操作和清空数据表操作都会释放空间。
```plsql
drop table table_name;
truncate table table_name;
```
使用delete删除的时候，MySQL并没有把数据文件删除，而是将数据文件的标识位删除，没有整理文件，因此不会彻底释放空间。被删除的数据将会被保存在一个链接清单中，当有新数据写入的时候，MySQL会利用这些已删除的空间再写入。即，删除操作会带来一些数据碎片，正是这些碎片在占用硬盘空间。
```sql
delete from table_name [where xxx];
```
如何优化表空间？todo
原文链接：[MySQL delete删除数据后,释放磁盘空间_mysql删除数据后释放空间-CSDN博客](https://blog.csdn.net/qq_33271461/article/details/131629400)


# mysql

## sql优化-所有批次内药品

最初的sql（省略了条件查询，实际还有where和having）

```sql
# 12.3
SELECT SQL_CALC_FOUND_ROWS mapping.id                                             as mapping_id,
                           min(mapping.batch_id)                                  as batch_id,
                           min(mapping.medical_code)                              as medical_code,
                           min(batch.batch_name)                                  as batch_name,
                           min(medical.id)                                        as id,
                           min(medical.type)                                      as type,
                           min(medical.register_name)                             as register_name,
                           min(medical.product_name)                              as product_name,
                           min(medical.medical_type)                              as medical_type,
                           min(medical.specs)                                     as specs,
                           min(medical.pack_material)                             as pack_material,
                           min(medical.min_pack_num)                              as min_pack_num,
                           min(medical.min_specs_unit)                            as min_specs_unit,
                           min(medical.min_pack_unit)                             as min_pack_unit,
                           min(medical.medical_manufacturer)                      as medical_manufacturer,
                           min(medical.approval_code)                             as approval_code,
                           min(medical.medical_standard_code)                     as medical_standard_code,
                           min(medical.catalogue_class_ab)                        as catalogue_class_ab,
                           min(medical.transformation_factor)                     as transformation_factor,
                           min(medical.corr_ybcode)                               as corr_ybcode,
                           min(mapping.update_time)                               as update_time,
                           GROUP_CONCAT(batch1.id order by batch1.id)             as common_name_id_batch_id_list,
                           GROUP_CONCAT(batch2.id order by batch2.id)             as association_id_batch_id_list,
                           GROUP_CONCAT(mark1.mark_name order by mark1.mark_name) as common_name_id,
                           GROUP_CONCAT(mark2.mark_name order by mark2.mark_name) as association_id
FROM batch_medical_mapping_template_1722261617659 AS mapping
         INNER JOIN batch_info_template_1722261617659 AS batch
                    ON mapping.batch_id = batch.id
         INNER JOIN medical_info_template_1722261617659 AS medical
                    ON mapping.medical_code = medical.medical_code
         left join batch_info_template_1722261617659 as batch1
                   on mapping.batch_id = batch1.id and batch1.field_type = 1
         left join batch_info_template_1722261617659 as batch2
                   on mapping.batch_id = batch2.id and batch2.field_type = 2
         left join remark_base as mark1 on batch1.field = mark1.id
         left join remark_base as mark2 on batch2.field = mark2.id
group by mapping.id
order by update_time desc, mapping_id desc
LIMIT 700000, 20;

SELECT FOUND_ROWS() AS total;
```

```ad-note
以上sql成为慢sql的原因：

- 大数据量下的group by
- 大数据量下的多表join
- 大数据量下分页，页码越靠后越慢
- 计数 SELECT SQL_CALC_FOUND_ROWS 很慢（类似于count(*)）
   - 当sql本身就很慢的时候SELECT SQL_CALC_FOUND_ROWS看不出差距了，感觉是mysql并行执行的
```

1）拆分sql：当没有having时，可以拆分sql，先分页再group by，同时也减少了join

> 12.3s -> 6.3s+0.4s

```sql
# 不带SQL_CALC_FOUND_ROWS：6.2s
# 带SQL_CALC_FOUND_ROWS：6.3s
SELECT SQL_CALC_FOUND_ROWS mapping.id                    as mapping_id,
                           mapping.batch_id              as batch_id,
                           mapping.medical_code          as medical_code,
                           batch.batch_name              as batch_name,
                           medical.id                    as id,
                           medical.type                  as type,
                           medical.register_name         as register_name,
                           medical.product_name          as product_name,
                           medical.medical_type          as medical_type,
                           medical.specs                 as specs,
                           medical.pack_material         as pack_material,
                           medical.min_pack_num          as min_pack_num,
                           medical.min_specs_unit        as min_specs_unit,
                           medical.min_pack_unit         as min_pack_unit,
                           medical.medical_manufacturer  as medical_manufacturer,
                           medical.approval_code         as approval_code,
                           medical.medical_standard_code as medical_standard_code,
                           medical.catalogue_class_ab    as catalogue_class_ab,
                           medical.transformation_factor as transformation_factor,
                           medical.corr_ybcode           as corr_ybcode,
                           mapping.update_time           as update_time
FROM batch_medical_mapping_template_1722261617659 AS mapping
         INNER JOIN batch_info_template_1722261617659 AS batch
                    ON mapping.batch_id = batch.id
         INNER JOIN medical_info_template_1722261617659 AS medical
                    ON mapping.medical_code = medical.medical_code
order by update_time desc, mapping_id desc
LIMIT 700000, 20;

# 根据第一步查出的20条数据，得到20个药品的medicalCode，再去执行另一半sql
# 0.4s
select mapping.medical_code                                   as medical_code,
       GROUP_CONCAT(batch1.id order by batch1.id)             as common_name_id_batch_id_list,
       GROUP_CONCAT(batch2.id order by batch2.id)             as association_id_batch_id_list,
       GROUP_CONCAT(mark1.mark_name order by mark1.mark_name) as common_name_id,
       GROUP_CONCAT(mark2.mark_name order by mark2.mark_name) as association_id
from batch_medical_mapping_template_1722261617659 as mapping
         left join batch_info_template_1722261617659 as batch1
                   on mapping.batch_id = batch1.id and batch1.field_type = 1
         left join batch_info_template_1722261617659 as batch2
                   on mapping.batch_id = batch2.id and batch2.field_type = 2
         left join remark_base as mark1 on batch1.field = mark1.id
         left join remark_base as mark2 on batch2.field = mark2.id
where medical_code in ('XJ01DCT070B001060100478',
                       'XJ01DCT070B001060105405',
                       'XJ01DCT070B001060105776',
                       'XJ01DCT070B001060200377',
                       'XJ01DCT070B001060200381',
                       'XJ01DCT070B001060202954',
                       'XJ01DCT070B001060300377',
                       'XJ01DCT070B001070100377',
                       'XJ01DCT070B001070100381',
                       'XJ01DCT070B001070104339',
                       'XJ01DCT070B001070105793',
                       'XJ01DCT070B001070200796',
                       'XJ01DCT070B001070204751',
                       'XJ01DCT070B001080100377',
                       'XJ01DCT070B001080102954',
                       'XJ01DCT070B001080104339',
                       'XJ01DCT070B001080200377',
                       'XJ01DCT070B001090100377',
                       'XJ01DCT070B001090100381',
                       'XJ01DCT070B001090104339')
group by mapping.medical_code;
```

2）优化分页：通过子查询优化分页

> 分页还有一种优化思路：正序查最后一页可以改为倒序查第一页，再把顺序调整回来，这个方法对于查两端的数据很快，但是中间的依旧很慢

```sql
# 不带SQL_CALC_FOUND_ROWS：4.4
# 带SQL_CALC_FOUND_ROWS：4.5
SELECT  mapping.id                    as mapping_id,
                           mapping.batch_id              as batch_id,
                           mapping.medical_code          as medical_code,
                           batch.batch_name              as batch_name,
                           medical.id                    as id,
                           medical.type                  as type,
                           medical.register_name         as register_name,
                           medical.product_name          as product_name,
                           medical.medical_type          as medical_type,
                           medical.specs                 as specs,
                           medical.pack_material         as pack_material,
                           medical.min_pack_num          as min_pack_num,
                           medical.min_specs_unit        as min_specs_unit,
                           medical.min_pack_unit         as min_pack_unit,
                           medical.medical_manufacturer  as medical_manufacturer,
                           medical.approval_code         as approval_code,
                           medical.medical_standard_code as medical_standard_code,
                           medical.catalogue_class_ab    as catalogue_class_ab,
                           medical.transformation_factor as transformation_factor,
                           medical.corr_ybcode           as corr_ybcode,
                           mapping.update_time           as update_time
FROM batch_medical_mapping_template_1722261617659 AS mapping
         INNER JOIN batch_info_template_1722261617659 AS batch
                    ON mapping.batch_id = batch.id
         INNER JOIN medical_info_template_1722261617659 AS medical
                    ON mapping.medical_code = medical.medical_code,
     (SELECT mapping.id as mapping_id
      FROM batch_medical_mapping_template_1722261617659 AS mapping
               INNER JOIN batch_info_template_1722261617659 AS batch
                          ON mapping.batch_id = batch.id
               INNER JOIN medical_info_template_1722261617659 AS medical
                          ON mapping.medical_code = medical.medical_code
      order by mapping.update_time desc, mapping_id desc
      LIMIT 700000, 20) as temp
where mapping.id = temp.mapping_id;

# 4.1s
SELECT count(*)  
FROM batch_medical_mapping_template_1722261617659 AS mapping  
         INNER JOIN batch_info_template_1722261617659 AS batch  
                    ON mapping.batch_id = batch.id  
         INNER JOIN medical_info_template_1722261617659 AS medical  
                    ON mapping.medical_code = medical.medical_code;
```

`count(*)` 决定了这个 sql 的上限

3）优化 `count(*)` #todo
- 并行执行
- `SELECT SQL_CALC_FOUND_ROWS`（网上不建议）
- 用 explain 获取模糊行数
- 新建一张专门用于 count 的表（多表不行）

4）索引优化：更新时间和 id 加联合索引

## 分布式锁

场景：确保版本串行创建

表结构：
- owner 保证不会误删锁
```sql
create table sys_lock
(
    id          bigint unsigned auto_increment comment 'ID'
        primary key,
    lock_key    varchar(255)                               not null comment '锁标识',
    status      tinyint unsigned default '0'               not null comment '状态 0.释放状态 1.锁状态',
    owner       varchar(255)                               null comment '锁归属标识',
    create_time timestamp        default CURRENT_TIMESTAMP not null comment '创建时间',
    update_time timestamp        default CURRENT_TIMESTAMP not null on update CURRENT_TIMESTAMP comment '更新时间',
    constraint `key`
        unique (lock_key)
);
```

业务代码：
- 在 finally 中确保锁一定会被释放
- 如果加锁的时候出现异常，别的版本加锁成功，通过 owner 标识版本，保证释放锁的时候不会释放掉别的版本的锁
```java
 // todo warn：异步创建药品表、批次表、药品-批次关联表
new Thread(() -> {
	sysLockService.waitLock("create_version", String.valueOf(addVersionId), 100000L);
	try {
		asyncCreateVersion(userId, addVersion, baseVersion, historyBatchMappingList);
	} finally {
		sysLockService.releaseLock("create_version", String.valueOf(addVersionId));
	}
}).start();
```

```java
@Service
public class SysLockService {

    @Resource
    private SysLockMapper sysLockMapper;

    public void waitLock(String key, String owner, long interval) {
        while (true) {
            int rows = sysLockMapper.getLock(key, owner);
            if (rows > 0) {
                break;
            }
            try {
                Thread.sleep(interval);
            } catch (InterruptedException e) {
                e.printStackTrace();
            }
        }
    }

    public void releaseLock(String key, String owner) {
        sysLockMapper.releaseLock(key, owner);
    }
}
```

```java
@Mapper
public interface SysLockMapper extends BaseMapper<SysLock> {

    @Update("update sys_lock set status=1,owner=#{owner} where lock_key=#{key} and status=0")
    int getLock(String key, String owner);

    @Update("update sys_lock set status=0,owner=null where lock_key=#{key} and status=1 and owner=#{owner}")
    void releaseLock(String key, String owner);

}
```

## 保证插入批次名称不会重复

```sql
select count(*) from batch where name='aaa' for update;

insert ...
```

批次名称没有索引，会直接锁表

注意：事务默认是rc，rc下没有间隙锁，如果给名称加索引，锁的是行，无法保证批次名称不会重复

可以在 `@Transactional` 注解上配置事务级别为 rr

## updatetime 相等导致分页数据重复

https://blog.csdn.net/Saintmm/article/details/122906957

## 逻辑删除和唯一性字段冲突

解决方法
- 逻辑删除字段 0 为未删除，已删除用时间戳
- 硬删除

> 不能用 id 和需要唯一索引的字段做唯一索引，那样就失去唯一索引的作用了

# knife4j
## 全局请求头设置
![](https://cdn.nlark.com/yuque/0/2024/png/12496339/1723183835408-e91ccfb6-6fc0-4566-b136-5c26cf578a5d.png#averageHue=%23f7f7f6&clientId=uc67694cd-4c42-4&from=paste&id=u0751d623&originHeight=430&originWidth=2428&originalType=url&ratio=2&rotation=0&showTitle=false&status=done&style=none&taskId=ua5759710-c1d6-467c-b794-9db3d329982&title=)
## 页面被拦截器拦截
viatris-web包下的LoginInterceptor默认会拦截所有页面
外部包无法修改，但是在配置文件中预留了对于拦截器的拦截路径和排除路径的配置
```java
@Override
public void addInterceptors(InterceptorRegistry registry) {
    InterceptorRegistration loginInterceptorRegistration = registry.addInterceptor(new LoginInterceptor());
    if (CollectionUtils.isNotEmpty(loginProperties.getPathPatterns())) {
        loginInterceptorRegistration.addPathPatterns(loginProperties.getPathPatterns());
    }
    if (CollectionUtils.isNotEmpty(loginProperties.getExcludePathPatterns())) {
        loginInterceptorRegistration.excludePathPatterns(loginProperties.getExcludePathPatterns());
    }
}
```
```java
@ConfigurationProperties(prefix = "viatris.login")
public class LoginProperties {
    /**
     * 需要鉴权的path pattern白名单
     */
    private List<String> pathPatterns;

    /**
     * 不需要鉴权的path pattern过滤名单
     */
    private List<String> excludePathPatterns;
}
```
在配置文件中指定排除路径
```java
viatris:
  login:
    excludePathPatterns: [ "/swagger-ui.html","/swagger-ui/**","/swagger-resources/**","/v3/api-docs/**","/webjars/**","/v2/**","/api","/api-docs","/api-docs/**","/doc.html/**" ]
```
## 过滤请求参数
@ApiIgnore

- 不能用于组合注解 Todo
# MP
## 动态表名拦截器
配合ThreadLocal动态给表名带上版本号
## 分页
配置分页插件，否则page对象很多字段不生效
## 配置
逻辑删除
主键自增策略 auto
## And or 嵌套拼接
https://blog.csdn.net/ymzhaobth/article/details/108094449
# MyBatis
## 执行DDL
用update标签可以执行ddl语句
```
<update id="createTable">
    create table ${newTableName} like ${tarTableName};
</update>
```
## 分页获取总条数
https://blog.csdn.net/qq_27610647/article/details/115550226
## 动态sql：where 后没有条件以及where后第一个and导致的报错
```
<where></where>
```
https://blog.csdn.net/qq_43842093/article/details/121728251

# JSR303注解校验
## 基本使用
https://blog.csdn.net/justry_deng/article/details/86571671
## CheckVersionValidator没有线程安全问题
# SpringSecurity
## 跳过认证、存放权限
 https://zhuanlan.zhihu.com/p/342755411
## 捕获过滤器抛出的自定义异常
https://blog.csdn.net/qq_40058629/article/details/112857305
## 放行/actuator
不然security会一直拦截，部署的时候通不过，报错
```
[2024-06-05 10:31:36.435] [http-nio-9080-exec-73] [ERROR] [o.a.c.c.C.[.[.[/].[dispatcherServlet]] [TID: N/A] - Servlet.service() for servlet [dispatcherServlet] in context with path [] threw exception [Request processing failed; nested exception is java.lang.IllegalStateException: getWriter() has already been called for this response] with root cause java.lang.IllegalStateException: getWriter() has already been called for this response at org.apache.catalina.connector.Response.getOutputStream(Response.java:538) ~[tomcat-embed-core-9.0.76.jar!/:na] at org.apache.catalina.connector.ResponseFacade.getOutputStream(ResponseFacade.java:178) ~[tomcat-embed-core-9.0.76.jar!/:na] at javax.servlet.ServletResponseWrapper.getOutputStream(ServletResponseWrapper.java:100) ~[tomcat-embed-core-9.0.76.jar!/:na] at javax.servlet.ServletResponseWrapper.getOutputStream(ServletResponseWrapper.java:100) ~[tomcat-embed-core-9.0.76.jar!/:na] at javax.servlet.ServletResponseWrapper.getOutputStream(ServletResponseWrapper.java:100) ~[tomcat-embed-core-9.0.76.jar!/:na] at org.springframework.security.web.util.OnCommittedResponseWrapper.getOutputStream(OnCommittedResponseWrapper.java:146) ~[spring-security-web-5.7.9.jar!/:5.7.9] at
```

# Spring
## @PostConstruct和静态代码块的区别
https://blog.csdn.net/weixin_51614339/article/details/123230849
```
@Slf4j
@Service
public class UploadFileInfoService extends ServiceImpl<UploadFileInfoMapper, UploadFileInfo> {

    private ExecutorService mainExecutor;
    private ExecutorService parseExecutor;

    @PostConstruct
    private void init() {
        mainExecutor = new ThreadPoolExecutor(5, 10, 0L, TimeUnit.MILLISECONDS,
                new LinkedBlockingQueue<Runnable>(64),
                new ThreadFactoryBuilder().setNameFormat("medical-upload-pool-%d").build(),
                new ThreadPoolExecutor.AbortPolicy());

        parseExecutor = new ThreadPoolExecutor(10, 100, 0L, TimeUnit.MILLISECONDS,
                new LinkedBlockingQueue<Runnable>(1024),
                new ThreadFactoryBuilder().setNameFormat("medical-parse-pool-%d").build(),
                new ThreadPoolExecutor.AbortPolicy());
    }
 }
```
# 内存溢出
## pdf导入内存溢出
## 标签接口超时
![](https://cdn.nlark.com/yuque/0/2024/png/12496339/1723183835486-0f3fd8f1-0829-479f-a855-b4b4abb85eb6.png#averageHue=%23fcfcfc&clientId=uc67694cd-4c42-4&from=paste&id=u1cc786de&originHeight=1186&originWidth=3428&originalType=url&ratio=2&rotation=0&showTitle=false&status=done&style=none&taskId=uec1bb698-f26e-46f4-9340-5b17f4737df&title=)
![](https://cdn.nlark.com/yuque/0/2024/png/12496339/1723183835487-1060cd2e-346c-4bd3-99c7-44b8fcc69400.png#averageHue=%23fefdfc&clientId=uc67694cd-4c42-4&from=paste&id=u04355887&originHeight=1304&originWidth=2990&originalType=url&ratio=2&rotation=0&showTitle=false&status=done&style=none&taskId=ue23c2118-0300-41e3-9fe5-c5c6fe7aac1&title=)
# Git 
## 分支管理策略
[分支管理](https://zfau4tzxp5.feishu.cn/wiki/EujqwowFFiDMrnkajQscwFSvnlr?from=from_copylink)
## Merge request
# 部署
构建
![](https://cdn.nlark.com/yuque/0/2024/png/12496339/1723183835579-b90e568a-b8b6-497d-9035-3523c62ec1ec.png#averageHue=%23fbfafa&clientId=uc67694cd-4c42-4&from=paste&id=u307cac85&originHeight=1372&originWidth=2266&originalType=url&ratio=2&rotation=0&showTitle=false&status=done&style=none&taskId=ub6840897-c153-48d8-9ae1-618fa5a94ea&title=)
基础镜像
![](https://cdn.nlark.com/yuque/0/2024/png/12496339/1723183835500-2d0d47aa-9e1c-44bf-913d-3c0eaea12790.png#averageHue=%23dddddb&clientId=uc67694cd-4c42-4&from=paste&id=ud220f417&originHeight=290&originWidth=1712&originalType=url&ratio=2&rotation=0&showTitle=false&status=done&style=none&taskId=ude497620-e0e1-4611-a610-87d8bfc93b7&title=)


# 并发问题
## 读-修改-写（_read-modify-write cycle_）
> 参考：[designing-data-intensive-applications.pdf](https://www.yuque.com/attachments/yuque/0/2024/pdf/12496339/1723174114384-13a49196-0e7d-4240-b266-7b20681485f8.pdf?_lake_card=%7B%22src%22%3A%22https%3A%2F%2Fwww.yuque.com%2Fattachments%2Fyuque%2F0%2F2024%2Fpdf%2F12496339%2F1723174114384-13a49196-0e7d-4240-b266-7b20681485f8.pdf%22%2C%22name%22%3A%22designing-data-intensive-applications.pdf%22%2C%22size%22%3A24975901%2C%22ext%22%3A%22pdf%22%2C%22source%22%3A%22%22%2C%22status%22%3A%22done%22%2C%22download%22%3Atrue%2C%22taskId%22%3A%22ue744d191-6aaf-4d59-9001-35c4d964199%22%2C%22taskType%22%3A%22upload%22%2C%22type%22%3A%22application%2Fpdf%22%2C%22__spacing%22%3A%22both%22%2C%22mode%22%3A%22title%22%2C%22id%22%3A%22sf016%22%2C%22margin%22%3A%7B%22top%22%3Atrue%2C%22bottom%22%3Atrue%7D%2C%22card%22%3A%22file%22%7D)

读-修改-写（_read-modify-write cycle_）可能会导致其他并行的 update 操作被覆盖丢失，就是 lost update problem
![img_v3_02dj_36efb47d-99d9-4a72-8f25-a106b8ccbcbg.jpg](https://cdn.nlark.com/yuque/0/2024/jpeg/12496339/1723174078822-758596fa-6c60-422e-a23b-539feb48a97a.jpeg#averageHue=%23e4e4e4&clientId=u70f7a44b-8216-4&from=paste&height=195&id=u099bd2d5&originHeight=390&originWidth=1420&originalType=binary&ratio=2&rotation=0&showTitle=false&size=221121&status=done&style=none&taskId=u0ee9c770-8e61-48fe-ba31-befead1ac0e&title=&width=710)
![img_v3_02dj_457c21af-11ab-43f4-a7d0-d0339a8004eg.jpg](https://cdn.nlark.com/yuque/0/2024/jpeg/12496339/1723174004006-44129376-4aa9-41f2-9179-56643ca20f03.jpeg#averageHue=%23f9f9f9&clientId=u70f7a44b-8216-4&from=paste&height=250&id=u3e4799f1&originHeight=500&originWidth=1408&originalType=binary&ratio=2&rotation=0&showTitle=false&size=113642&status=done&style=none&taskId=ucd489428-bdf4-41da-b79b-53b6e354615&title=&width=704)
# 未解决
## BeanUtils拷贝失效
apache工具包下的BeanUtils拷贝mp的page对象失败，原因未知
改用spring提供的BeanUtils
https://blog.csdn.net/qq_27753441/article/details/116659975
## 循环依赖
配置开启允许循环依赖
